{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication  \n",
    "from azureml.core.compute import ComputeTarget, AmlCompute  \n",
    "from azureml.core.compute_target import ComputeTargetException  \n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to your workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of Azure ML 1.53.0 to work with wks-pablo-ts\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config(path= './config.json')\n",
    "print(f'Version of Azure ML {azureml.core.VERSION} to work with {ws.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload the datastore and create the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "'overwrite' is set to True. Any file already present in the target will be overwritten.\n",
      "Uploading files from 'c:/Users/pablo.prieto/Desktop/TS_Azure/data' to 'sp500-data/'\n",
      "Creating new dataset\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if 'sp500_dataset' not in  ws.datasets:\n",
    "\n",
    "    Dataset.File.upload_directory(src_dir='data',\n",
    "                                target=DataPath(default_ds, 'sp500-data/'),\n",
    "                                overwrite= True\n",
    "                                )\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'sp500-data/*.csv'))\n",
    "\n",
    "    data_set =(Dataset\n",
    "                .Tabular\n",
    "                .from_delimited_files(path=(default_ds,'sp500-data/*.csv'))\n",
    "                ) \n",
    "\n",
    "    try:\n",
    "        data_set = data_set.register( workspace=ws,\n",
    "                                    name =  'sp500 dataset',\n",
    "                                    description =  'sp500 data',\n",
    "                                    tags = {'format':'CSV'},\n",
    "                                    create_new_version=True\n",
    "                                    )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create scripts for pipeline steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp500_prep folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "preparing_folder = 'sp500_prep'\n",
    "os.makedirs(preparing_folder,exist_ok=True)\n",
    "print(preparing_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The pipeline contains two scripts:**\n",
    "\n",
    "The first script named *prep_sp500.py* loads and prepares the data that will be exported to the next script.\n",
    "\n",
    "The second script so-called *training_sp500.py*  imports the preprocesed data in order to train and to register a model based on SARIMAX structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sp500_prep/prep_sp500.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $preparing_folder/prep_sp500.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "\n",
    "# Get_parameter\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input-data', type = str, dest = 'raw_dataset_id' , help = 'raw dataset')\n",
    "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "# Get run context\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Loading data.....\")\n",
    "\n",
    "df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "df[\"DATE\"] =  pd.to_datetime(df[\"DATE\"])\n",
    "df['SP500']= pd.to_numeric(df['SP500'], errors='coerce')\n",
    "df = pd.DataFrame(df)\n",
    "df = df.dropna()               \n",
    "               \n",
    "row_sp500 = len(df)\n",
    "\n",
    "run.log('Number_rows', row_sp500)\n",
    "\n",
    "# Save the prepped data\n",
    "print(\"Saving Data...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder,'SP500.csv')\n",
    "df.to_csv(save_path, index = False, header=True)\n",
    "\n",
    "# End the run\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp500_training folder created\n"
     ]
    }
   ],
   "source": [
    "training_folder = 'sp500_training'\n",
    "os.makedirs(training_folder,exist_ok=True)\n",
    "print(training_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sp500_training/training_sp500.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/training_sp500.py\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from azureml.core import Run, Model\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_ARIMA(endog: Union[pd.Series, list], order_list: list, d: int) -> pd.DataFrame:\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for order in order_list:\n",
    "        try: \n",
    "            model = SARIMAX(endog, order=(order[0], d, order[1]), simple_differencing=False).fit(disp=False)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        results.append([order, aic])\n",
    "        \n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.columns = ['(p,q)', 'AIC']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, d: int, order: tuple) -> list:\n",
    "    \n",
    "    total_len = train_len + horizon\n",
    "    pred_ARMA = []\n",
    "    \n",
    "    for i in range(train_len, total_len, window):\n",
    "        model = SARIMAX(endog=df, order=(order[0],d,order[1]))\n",
    "        res = model.fit(disp=False)\n",
    "        predictions = res.get_prediction(0, i + window - 1)\n",
    "        oos_pred = predictions.predicted_mean.iloc[-window:]\n",
    "        pred_ARMA.extend(oos_pred)\n",
    "        \n",
    "    return pred_ARMA\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-data\",\n",
    "                    type = str, \n",
    "                    dest = 'training_data', \n",
    "                    help = 'training data' \n",
    "                    )\n",
    "\n",
    "args = parser.parse_args()\n",
    "training_data = args.training_data\n",
    "\n",
    "\n",
    "# Get the experiment run context\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "file_path = os.path.join(training_data,'SP500.csv')\n",
    "df = (pd.read_csv(file_path, parse_dates = [\"DATE\"])\n",
    "        .set_index(\"DATE\")\n",
    "     )\n",
    "\n",
    "shape = df.shape\n",
    "run.log(\"DF shape\", shape)\n",
    "\n",
    "d = 0\n",
    "p_value = adfuller(df)[1]\n",
    "for i in range(5):\n",
    "    \n",
    "    if p_value > 0.05:\n",
    "        eps_diff = np.diff(df['SP500'], n = i + 1 )\n",
    "        p_value = adfuller(eps_diff)[1]\n",
    "        d = d  + 1\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "print(f\"The value of integration d is: {d}\")\n",
    "\n",
    "\n",
    "# Combination of multiple values for p and q from zero to five\n",
    "n = 5\n",
    "order_list = [(p,q) for p in range(0,n+1) for q in range(0,n+1)]\n",
    "\n",
    "\n",
    "limit = np.int(0.2*len(df))\n",
    "train = df.iloc[:-limit]\n",
    "test = df.iloc[-limit:]\n",
    "\n",
    "result_df = optimize_ARIMA(train, order_list, d)\n",
    "order = result_df.iloc[0,0]\n",
    "\n",
    "model = SARIMAX(train, order=(order[0],d,order[1]), simple_differencing=False)\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_LEN = len(train)\n",
    "HORIZON = len(test)\n",
    "WINDOW = 1\n",
    "pred_ARMA = rolling_forecast(df, TRAIN_LEN, HORIZON, WINDOW,d,order)\n",
    "\n",
    "df_test  = pd.DataFrame(test)\n",
    "df_test[\"pred_fort\"] = pred_ARMA\n",
    "\n",
    "rmse = mean_squared_error(df_test['SP500'], df_test[\"pred_fort\"])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_test['SP500'].iloc[:30], label='Actual')\n",
    "ax.plot(df_test[\"pred_fort\"].iloc[:30], label='Test')\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "run.log_image('Test', plot = fig)\n",
    "print('RMSE:', rmse)\n",
    "run.log('RMSE', np.float(rmse))\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "print(\"Saving model...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'sp500_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "            model_path = model_file,\n",
    "            model_name = 'sp500_model',\n",
    "            tags={'Training context':'Pipeline'},\n",
    "            properties={'RMSE': np.float(rmse)})\n",
    "\n",
    "run.complete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_cmp = \"prediction-sp500\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_cmp)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(workspace=ws, \n",
    "                                                name=cluster_cmp, \n",
    "                                                provisioning_configuration=compute_config\n",
    "                                                )\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sp500_training/experiment_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/experiment_env.yml\n",
    "\n",
    "name: experiment_env\n",
    "\n",
    "dependencies:\n",
    "- python=3.8.10\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- statsmodels\n",
    "- pandas\n",
    "- typing\n",
    "- pip\n",
    "- pip:\n",
    "   - azureml-defaults\n",
    "   - pyarrow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification(\"experiment_env\", training_folder + \"/experiment_env.yml\")\n",
    "\n",
    "# Register the environment \n",
    "experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'experiment_env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and Run Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Get the training dataset\n",
    "sp500_ds = ws.datasets.get(\"sp500 dataset\")\n",
    "\n",
    "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
    "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
    "\n",
    "prep_step = PythonScriptStep(name= \"Prepare Data\",\n",
    "                             source_directory = preparing_folder,\n",
    "                             script_name = \"prep_sp500.py\",\n",
    "                             arguments = ['--input-data', sp500_ds.as_named_input('raw_data'),\n",
    "                                             '--prepped-data', prepped_data],\n",
    "                            compute_target = pipeline_cluster,\n",
    "                            runconfig = pipeline_run_config,\n",
    "                            allow_reuse = True\n",
    "                             )\n",
    "\n",
    "# Step 2, run the training script\n",
    "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = training_folder,\n",
    "                                script_name = \"training_sp500.py\",\n",
    "                                arguments = ['--training-data', prepped_data.as_input()],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [prep_step, train_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'mslearn-sp500-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Depict the metrics recorded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Register Model :\n",
      "\t DF shape : (1258, 1)\n",
      "\t Test : aml://artifactId/ExperimentRun/dcid.b81ed40e-dd54-4d45-85e2-31ec81cd6a75/Test_1698363614.png\n",
      "\t RMSE : 1737.3741505628946\n",
      "Prepare Data :\n",
      "\t Number_rows : 1258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for run in pipeline_run.get_children():\n",
    "    print(run.name, ':')\n",
    "    metrics = run.get_metrics()\n",
    "    for metric_name in metrics:\n",
    "        print('\\t',metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>sp500-training-pipeline</td><td><a href=\"https://ml.azure.com/pipelines/626a1fc8-ea19-492c-a260-40949e7481b4?wsid=/subscriptions/b9e9fb4d-b0bc-475b-9c01-affa25c516f1/resourcegroups/rsc-pablo-ts/workspaces/wks-pablo-ts\" target=\"_blank\" rel=\"noopener\">626a1fc8-ea19-492c-a260-40949e7481b4</a></td><td>Active</td><td><a href=\"https://westus.api.azureml.ms/pipelines/v1.0/subscriptions/b9e9fb4d-b0bc-475b-9c01-affa25c516f1/resourceGroups/rsc-pablo-ts/providers/Microsoft.MachineLearningServices/workspaces/wks-pablo-ts/PipelineRuns/PipelineSubmit/626a1fc8-ea19-492c-a260-40949e7481b4\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: sp500-training-pipeline,\n",
       "Id: 626a1fc8-ea19-492c-a260-40949e7481b4,\n",
       "Status: Active,\n",
       "Endpoint: https://westus.api.azureml.ms/pipelines/v1.0/subscriptions/b9e9fb4d-b0bc-475b-9c01-affa25c516f1/resourceGroups/rsc-pablo-ts/providers/Microsoft.MachineLearningServices/workspaces/wks-pablo-ts/PipelineRuns/PipelineSubmit/626a1fc8-ea19-492c-a260-40949e7481b4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Publish the pipeline from the run\n",
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"sp500-training-pipeline\", description=\"Trains sp500 model\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
